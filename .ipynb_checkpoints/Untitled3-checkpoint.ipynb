{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xed_9hEFqyHA",
    "outputId": "905b2dac-a7da-411d-a0e3-288486bfc70d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import brown\n",
    "from nltk.util import everygrams\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "data = pd.read_csv('dataset/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Galaxy Gear có thể điều khiển TV thông minh</td>\n",
       "      <td>Công nghệ</td>\n",
       "      <td>Mẫu đồng hồ thông minh Galaxy Gear của Samsung...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nhật – Mỹ định hội đàm chớp nhoáng tại Singapo...</td>\n",
       "      <td>Thế giới</td>\n",
       "      <td>&gt;&gt; Phó Tổng thống Mỹ công du châu Á, bàn về Bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hàng ngàn học sinh miền núi Nghệ An bỏ học</td>\n",
       "      <td>Giáo dục</td>\n",
       "      <td>Hàng ngàn học sinh sau khi tốt nghiệp THCS đã ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'Cô ấy bỏ em vì... em gái trượt đại học?'</td>\n",
       "      <td>Đời sống</td>\n",
       "      <td>Mr Búp Bê (BB): Hay là bạn phải đến kèm cho cô...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cậu bé vượt qua Face ID của iPhone ngang anh Q...</td>\n",
       "      <td>Công nghệ</td>\n",
       "      <td>Trang Wired mới đây đưa thông tin cậu bé Ammar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title   category  \\\n",
       "0        Galaxy Gear có thể điều khiển TV thông minh  Công nghệ   \n",
       "1  Nhật – Mỹ định hội đàm chớp nhoáng tại Singapo...   Thế giới   \n",
       "2         Hàng ngàn học sinh miền núi Nghệ An bỏ học   Giáo dục   \n",
       "3          'Cô ấy bỏ em vì... em gái trượt đại học?'   Đời sống   \n",
       "4  Cậu bé vượt qua Face ID của iPhone ngang anh Q...  Công nghệ   \n",
       "\n",
       "                                             content  \n",
       "0  Mẫu đồng hồ thông minh Galaxy Gear của Samsung...  \n",
       "1  >> Phó Tổng thống Mỹ công du châu Á, bàn về Bi...  \n",
       "2  Hàng ngàn học sinh sau khi tốt nghiệp THCS đã ...  \n",
       "3  Mr Búp Bê (BB): Hay là bạn phải đến kèm cho cô...  \n",
       "4  Trang Wired mới đây đưa thông tin cậu bé Ammar...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uWfiPjOCrJov",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tokenize the content of each article\n",
    "data['tokenized_content'] = data['content'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [Mẫu, đồng, hồ, thông, minh, Galaxy, Gear, của...\n",
       "1        [>, >, Phó, Tổng, thống, Mỹ, công, du, châu, Á...\n",
       "2        [Hàng, ngàn, học, sinh, sau, khi, tốt, nghiệp,...\n",
       "3        [Mr, Búp, Bê, (, BB, ), :, Hay, là, bạn, phải,...\n",
       "4        [Trang, Wired, mới, đây, đưa, thông, tin, cậu,...\n",
       "                               ...                        \n",
       "51994                                        [VIỆT, AN, .]\n",
       "51995    [Chú, gấu, túi, bị, kẹt, trong, bánh, xe, ô, t...\n",
       "51996    [Video, đập, chổi, vào, đầu, cá, mập, trắng, k...\n",
       "51997    [Máy, ảnh, của, vệ, tinh, đã, chụp, được, nhữn...\n",
       "51998    [Cô, nàng, gấu, trúc, Funi, rất, hào, hứng, kh...\n",
       "Name: tokenized_content, Length: 51999, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokenized_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate the frequency distribution of each token in the corpus\n",
    "fdist = nltk.FreqDist(data['tokenized_content'].explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate the total number of tokens in the corpus\n",
    "total_tokens = len(data['tokenized_content'].explode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build a unigram model using Laplace smoothing\n",
    "def unigram_model(tokenized_text, vocab_size, alpha=1):\n",
    "    freq = nltk.FreqDist(tokenized_text)\n",
    "    model = {token: (freq[token] + alpha) / (len(tokenized_text) + alpha * vocab_size) for token in freq}\n",
    "    return model\n",
    "\n",
    "unigram = unigram_model(data['tokenized_content'].explode(), len(fdist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ObU6MKkQrNk2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_next_token(model, input_tokens):\n",
    "    # initialize the maximum probability and next token\n",
    "    max_prob = 0\n",
    "    next_token = None\n",
    "    # loop through all tokens in the model\n",
    "    for token in model.keys():\n",
    "        # check if the token starts with the input_tokens\n",
    "        if token.startswith(tuple(input_tokens)):\n",
    "            prob = model[token]\n",
    "            if prob > max_prob:\n",
    "                max_prob = prob\n",
    "                next_token = token\n",
    "    # return the next token\n",
    "    return next_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_text(model, input_string, n):\n",
    "    text = nltk.word_tokenize(input_string)\n",
    "    for i in range(n):\n",
    "        text.append(generate_next_token(unigram, input_string))\n",
    "        input_string = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JJUVBwwbrV2m",
    "outputId": "7a7ca6b2-ec16-4b4a-f547-6cbb36d0846c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Đơn', 'hàng', 'chưa', 'của', 'của', 'của', 'của', 'của', 'của', 'của', 'của', 'của', 'của']\n"
     ]
    }
   ],
   "source": [
    "# join the tokens into a string\n",
    "input_string = \"Đơn hàng chưa\"\n",
    "\n",
    "# generate the next text\n",
    "print(generate_text(unigram, input_string, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m data \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# create 2-gram model\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m words \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m()\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(words)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py:5902\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5895\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5896\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   5897\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   5898\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   5899\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5900\u001b[0m ):\n\u001b[0;32m   5901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 5902\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# load data\n",
    "with open('vietnamese_corpus.txt', 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "\n",
    "# create n-gram model\n",
    "n = 3\n",
    "words = data.split()\n",
    "model = {}\n",
    "vocab = set(words)\n",
    "for i in range(len(words)-n):\n",
    "    ngram = tuple(words[i:i+n])\n",
    "    if ngram not in model:\n",
    "        model[ngram] = {w: 1 for w in vocab}\n",
    "    model[ngram][words[i+n]] += 1\n",
    "\n",
    "# generate text\n",
    "start_ngram = random.choice(list(model.keys()))\n",
    "text = ' '.join(start_ngram) + ' '\n",
    "for i in range(50):\n",
    "    curr_ngram = tuple(text.split()[-n:])\n",
    "    if curr_ngram not in model:\n",
    "        next_word = random.choice(list(vocab))\n",
    "    else:\n",
    "        freqs = model[curr_ngram]\n",
    "        total_freq = sum(freqs.values())\n",
    "        probs = {w: (freqs[w] + 1) / (total_freq + len(vocab)) for w in vocab}\n",
    "        next_word = random.choices(list(vocab), weights=list(probs.values()))[0]\n",
    "    text += next_word + ' '\n",
    "\n",
    "print(text)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
